{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoded(y_train, classes = 0):\n",
    "    one_hot = np.zeros((len(y_train), classes))\n",
    "    array = y_train\n",
    "    if type(array) is not np.ndarray: \n",
    "        array = np.array(y_train)\n",
    "    for i in range(len(y_train)):\n",
    "        one_hot[i][array[i]] = 1.0\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self):\n",
    "        self.num_layers = 0\n",
    "        self.layers_info = {}\n",
    "        self.layers = []\n",
    "        self.net_sum = []\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.jacobian_weights = []\n",
    "        self.jacobian_biases = []\n",
    "    \n",
    "    # Add a fully connected layer\n",
    "    def add_dense_layer(self, size, input_size = 0, activation = 'linear'):\n",
    "        self.num_layers += 1\n",
    "        self.layers_info['Layer_'+str(self.num_layers)] = {\n",
    "            'size': size,\n",
    "            'activation': activation\n",
    "        }\n",
    "        self.layers.append(np.ones(size))\n",
    "        self.net_sum.append(np.ones(size))\n",
    "        if input_size != 0:\n",
    "            self.weights.append(np.ones((size, input_size)))\n",
    "        else:\n",
    "            self.weights.append(np.ones((size, self.layers_info['Layer_'+str(self.num_layers-1)]['size'])))\n",
    "        self.biases.append(np.ones(size))\n",
    "        \n",
    "    def mse(self, true, pred):\n",
    "        return ((true-pred)**2)/2\n",
    "        \n",
    "    # Selection of Loss Function and Optimization function\n",
    "    def set_parameters(self, lr = 0.01, loss = 'mse'):\n",
    "        if loss == 'mse':\n",
    "            self.loss = loss\n",
    "            self.loss_function = self.mse\n",
    "        self.lr = lr\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + (np.e ** -x))\n",
    "    \n",
    "    def relu(self, x):\n",
    "        return max(0, x)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        exp = np.exp(x)\n",
    "        return np.true_divide(exp, sum(exp)).transpose()\n",
    "    \n",
    "    def activation(self, net_sum, activation):\n",
    "        if activation == 'sigmoid':\n",
    "            activation_function = self.sigmoid\n",
    "            output_vector = np.array([activation_function(i) for i in net_sum]).transpose()\n",
    "        elif activation == 'relu':\n",
    "            activation_function = self.relu\n",
    "            output_vector = np.array([activation_function(i) for i in net_sum]).transpose()\n",
    "        elif activation == 'softmax':\n",
    "            activation_function = self.softmax\n",
    "            output_vector = activation_function(net_sum)\n",
    "        return output_vector\n",
    "    \n",
    "    # Forward Propogation\n",
    "    def neural_network_output(self, record):\n",
    "        input_vector = record.transpose()\n",
    "        output_vector = None\n",
    "        for i in range(len(self.layers)):\n",
    "            # y = aW + b\n",
    "            self.net_sum[i] = np.matmul(self.weights[i], input_vector) + self.biases[i].transpose()\n",
    "            self.layers[i] = self.activation(self.net_sum[i], self.layers_info['Layer_'+str(i+1)]['activation'])\n",
    "            input_vector = self.layers[i]\n",
    "        output_vector = self.layers[len(self.layers)-1]\n",
    "        return output_vector\n",
    "    \n",
    "    # For output layer\n",
    "    # delta = dE/dnet_sum_output = dE/doutput * doutput/dnet_sum_output\n",
    "    # dE/doutput\n",
    "    def output_loss_derivative(self, true, pred):\n",
    "        if self.loss == 'mse':\n",
    "            return (true - pred)\n",
    "    \n",
    "    # doutput/dnet_sum_output\n",
    "    def activation_derivative(self, output_vector_i, net_sum_i, activation):\n",
    "        if activation == 'relu':\n",
    "            return np.array([1 if i>0 else 0 for i in net_sum_i])\n",
    "        elif activation == 'sigmoid':\n",
    "            return output_vector_i * (1 - output_vector_i)\n",
    "        elif activation == 'softmax':\n",
    "            return output_vector_i * (1 - output_vector_i)\n",
    "        else:\n",
    "            return output_vector_i * (1 - output_vector_i)\n",
    "    \n",
    "    # For hidden layer\n",
    "    # dE/dnet_sum_hidden = dotproduct(weights_ji, delta_o)\n",
    "    def hidden_loss_derivative(self, grad_next, layer_number, num_of_neurons_next):\n",
    "        grad = []\n",
    "        for i in range(num_of_neurons_next):\n",
    "            weights_next = self.weights[layer_number+1][:, i]\n",
    "            grad.append(np.dot(weights_next, grad_next))\n",
    "        return np.array(grad)\n",
    "    \n",
    "    # Training using Backpropogation\n",
    "    def train(self, X_train, y_train, epochs = 1, batch_size = 0):\n",
    "        \n",
    "        # initializing Jacobian\n",
    "        for i in range(self.num_layers):\n",
    "            size = self.layers_info['Layer_'+str(i+1)]['size']            \n",
    "            self.jacobian_weights.append(np.zeros((size)))\n",
    "            self.jacobian_biases.append(np.zeros((size)))\n",
    "        \n",
    "        # Creation of batches\n",
    "        if batch_size == 0:\n",
    "            batch_size = len(y_train)\n",
    "        batches = int(abs(len(X_train) / batch_size))\n",
    "        X_train = np.array_split(X_train, batches)\n",
    "        y_train = np.array_split(y_train, batches)\n",
    "        \n",
    "        while epochs: \n",
    "            for batch in range(batches):\n",
    "                for record, label in zip(X_train[batch], y_train[batch]):\n",
    "                    true_output = np.array([label])\n",
    "                    predicted_output = self.neural_network_output(np.array(record))\n",
    "\n",
    "                    # gradient of output layer\n",
    "                    output_vector_i = self.layers[self.num_layers-1]\n",
    "                    net_sum_i = self.net_sum[self.num_layers-1]\n",
    "                    activation = self.layers_info['Layer_'+str(self.num_layers)]['activation']\n",
    "                    # derivative of output loss * derivetive of activation\n",
    "                    grad_o = self.output_loss_derivative(true_output, predicted_output) * self.activation_derivative(output_vector_i, net_sum_i, activation)\n",
    "                    \n",
    "                    # Jacobian Output Layer - Jw = input * grad_o and Jb = 1.0 * grad_o\n",
    "                    if self.num_layers-2 < 0:\n",
    "                        self.jacobian_weights[self.num_layers-1] += np.array(record) * grad_o\n",
    "                    self.jacobian_weights[self.num_layers-1] += grad_o[:, None] @ self.layers[self.num_layers-2][None, :]\n",
    "                    self.jacobian_biases[self.num_layers-1] += grad_o\n",
    "                    \n",
    "                    # gradient of hidden layer\n",
    "                    grad_next = grad_o\n",
    "                    for i in range(self.num_layers-2, -1, -1):\n",
    "                        output_vector_i = self.layers[i]\n",
    "                        net_sum_i = self.layers[i]\n",
    "                        activation = self.layers_info['Layer_'+str(i+1)]['activation']\n",
    "                        num_of_neurons_next = self.layers_info['Layer_'+str(i+2)]['size']\n",
    "                        \n",
    "                        # derivative of hidden loss * derivetive of activation\n",
    "                        grad_h = self.hidden_loss_derivative(grad_next, i, num_of_neurons_next) * self.activation_derivative(output_vector_i, net_sum_i, activation)\n",
    "                        \n",
    "                        # Jacobian Hidden layer - Jw = input * grad_next and Jb = 1.0 * grad_next\n",
    "                        if i-1 < 0:\n",
    "                            self.jacobian_weights[i] += np.array(record) * grad_next\n",
    "                        self.jacobian_weights[i] += grad_next @ self.layers[i-1]\n",
    "                        self.jacobian_biases[i] += grad_next\n",
    "                        \n",
    "                        # change the gradient\n",
    "                        grad_next = grad_h\n",
    "                \n",
    "                # Divide accumulated jacobian by number of records in the batch\n",
    "                for i in range(self.num_layers):\n",
    "                    self.jacobian_weights[i] = np.true_divide(self.jacobian_weights[i], len(record))\n",
    "                    self.jacobian_biases[i] = np.true_divide(self.jacobian_biases[i], len(record))\n",
    "                \n",
    "                # Update weights and biases\n",
    "                for i in range(self.num_layers-1, -1, -1):\n",
    "                    self.weights[i] += self.lr * self.jacobian_weights[i]\n",
    "                    self.biases[i] += self.lr * self.jacobian_biases[i]\n",
    "            \n",
    "            # Loss Calculation\n",
    "            loss = 0\n",
    "            for record, label in zip(X_train[batch], y_train[batch]):\n",
    "                true_output = label\n",
    "                predicted_output = self.neural_network_output(record)\n",
    "                loss += self.loss_function(true_output, predicted_output)\n",
    "            print('Loss: ',loss/(len(X_train)))\n",
    "            \n",
    "            epochs -= 1\n",
    "    \n",
    "    # Predict\n",
    "    def predict(self, X_test):\n",
    "        predictions = []\n",
    "        classes = self.layers_info['Layer_'+str(self.num_layers-1)]['size']\n",
    "        for record in X_test:\n",
    "            predicted_output = self.neural_network_output(record)\n",
    "            if classes == 1:\n",
    "                if predicted_output >= 0.5:\n",
    "                    predictions.append(1)\n",
    "                else:\n",
    "                    predictions.append(0)\n",
    "            else:\n",
    "                predict = np.argmax(predicted_output)\n",
    "                predictions.append(predict)\n",
    "        return predictions\n",
    "            \n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0]\n",
      "[1. 0.]\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "\n",
    "model.add_dense_layer(2, 2, activation='relu')\n",
    "model.add_dense_layer(2, activation='softmax')\n",
    "\n",
    "X_train = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "y_train = [0, 1, 1, 0]\n",
    "y_train = one_hot_encoded(y_train, classes = 2)\n",
    "\n",
    "x = model.train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (1,) and (4,) not aligned: 1 (dim 0) != 4 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-203-f5bfe203b0b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mse'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-202-3c10508f1c64>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X_train, y_train, epochs, batch_size)\u001b[0m\n\u001b[1;32m    130\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjacobian_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrad_o\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjacobian_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_o\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjacobian_biases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mgrad_o\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (1,) and (4,) not aligned: 1 (dim 0) != 4 (dim 0)"
     ]
    }
   ],
   "source": [
    "tmodel = Model()\n",
    "\n",
    "tmodel.add_dense_layer(4, 3, activation='sigmoid')\n",
    "tmodel.add_dense_layer(1, activation='sigmoid')\n",
    "\n",
    "X_train = np.array([[0, 0, 1], [0, 1, 1], [1, 0, 1], [0, 1, 0], [1, 0, 0], [1, 1, 1], [0, 0, 0]])\n",
    "y_train = np.array([0, 1, 1, 1, 1, 0, 0])\n",
    "\n",
    "tmodel.set_parameters(lr=1, loss='mse')\n",
    "\n",
    "x = tmodel.train(X_train, y_train, batch_size=1, epochs=60000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmodel.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.1931976 , -0.25570842],\n",
       "        [-0.1931976 , -0.25570842]]), array([[-0.0350957, -0.0350957]])]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmodel.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3, 4],\n",
       "       [2, 4, 6, 8]])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1])\n",
    "b = np.array([1,2,3,4])\n",
    "a[:,None] @ b[None,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
